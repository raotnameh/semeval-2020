{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import pprint\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import os \n",
    "import re\n",
    "import math\n",
    "from tqdm import tqdm_notebook,tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ds_loader_label(filename):\n",
    "  with open(filename, 'r') as fp:\n",
    "    lines = [line.strip() for line in fp]   \n",
    "  posts, post = [], []\n",
    "  for line in tqdm_notebook(lines):\n",
    "    probs=[]\n",
    "    if line :\n",
    "        annotations = line.split(\"\\t\")[2]\n",
    "        # reading probabilities from the last column and also normalaize it by div on 9\n",
    "        annotations = np.array(annotations.split('|'))\n",
    "        probs.append(sum(annotations=='O'))\n",
    "        probs.append(sum(annotations=='B'))\n",
    "        probs.append(sum(annotations=='I'))\n",
    "        probs = [probs[0],probs[2]+probs[1]]\n",
    "        probs = [i/9 for i in probs]\n",
    "        post.append(probs)\n",
    "    elif post:\n",
    "        posts.append(post)\n",
    "        post = []\n",
    "  # a list of lists of words/ labels\n",
    "  return posts\n",
    "\n",
    "\n",
    "def ds_loader_token(filename):\n",
    "      with open(filename, 'r') as fp:\n",
    "        lines = [line.strip() for line in fp]   \n",
    "      posts, post = [], []\n",
    "      ids,id =[],[]  \n",
    "      for line in tqdm_notebook(lines):\n",
    "          if line:\n",
    "              words = line.split(\"\\t\")[1]\n",
    "              word_id=line.split(\"\\t\")[0]  \n",
    "              # print(\"words: \", words)\n",
    "              post.append(words)\n",
    "              id.append(word_id)  \n",
    "          elif post:\n",
    "              posts.append(post)\n",
    "              ids.append(id)  \n",
    "              id=[]  \n",
    "              post = []\n",
    "      # a list of lists of words/ labels\n",
    "      if len(post):   \n",
    "            posts.append(post)\n",
    "            ids.append(id)  \n",
    "      return posts,ids\n",
    "\n",
    "train_tokens,train_ids=ds_loader_token('/home/pradyumna/emph_data/train.txt')\n",
    "train_label=ds_loader_label('/home/pradyumna/emph_data/train.txt')\n",
    "\n",
    "dev_tokens,dev_ids=ds_loader_token('/home/pradyumna/emph_data/dev.txt')\n",
    "#dev_tokens,dev_ids=ds_loader_token('/media/nas_mount/Sarthak/sarthak/semeval/shuffle_dev/shuffle_dev_5.txt')\n",
    "dev_label=ds_loader_label('/home/pradyumna/emph_data/dev.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length=max([len(token) for token in train_tokens])+2\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id, valid_ids=None, label_mask=None):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.valid_ids = valid_ids\n",
    "        self.label_mask = label_mask\t\t\n",
    "\n",
    "def convert_examples_to_features(tokens_set, labels_set, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    #label_map = {label: i for i, label in enumerate(label_list, 1)}\n",
    "\n",
    "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
    "    for index in tqdm_notebook(range(len(tokens_set)),desc=\"Converting examples to features\"):\n",
    "        textlist = tokens_set[index] #example.text_a.split(' ')\n",
    "        labellist = labels_set[index]\n",
    "        input_id, input_mask, segment_id,label = convert_single_example(\n",
    "            textlist, labellist,max_seq_length,tokenizer\n",
    "        )\n",
    "        input_ids.append(input_id)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "        labels.append(label)\n",
    "    return (\n",
    "        np.array(input_ids),\n",
    "        np.array(input_masks),\n",
    "        np.array(segment_ids),\n",
    "        np.array(labels)\n",
    "    )\n",
    "\n",
    "def convert_single_example(textlist, labellist, max_seq_length,tokenizer):\n",
    "  tokens = []\n",
    "  labels = []\n",
    "  for i, word in enumerate(textlist):\n",
    "      token = tokenizer.tokenize(word)\n",
    "      if token[0]=='▁' and len(token)>1:\n",
    "        tokens.append(token[1])  \n",
    "      else :  \n",
    "        tokens.append(token[0])  \n",
    "      labels.append(labellist[i])\n",
    "  if len(tokens) >= max_seq_length - 1:\n",
    "      tokens = tokens[0:(max_seq_length - 2)]\n",
    "      labels = labels[0:(max_seq_length - 2)]\n",
    "  ntokens = []\n",
    "  segment_ids = []\n",
    "  #ntokens.append(\"[CLS]\")\n",
    "  #segment_ids.append(0)\n",
    "  labels.append([1,0])\n",
    "  for i, token in enumerate(tokens):\n",
    "      ntokens.append(token)\n",
    "      segment_ids.append(0)\n",
    "  #ntokens.append(\"[SEP]\")\n",
    "  segment_ids.append(0)  \n",
    "  segment_ids.append(0)\n",
    "  labels.append([1,0])\n",
    "  input_ids = tokenizer.convert_tokens_to_ids(ntokens)\n",
    "  input_ids.append(4)\n",
    "  input_ids.append(3)  \n",
    "  input_mask = [1] * len(input_ids)\n",
    "  while len(input_ids) < max_seq_length:\n",
    "      input_ids.insert(0,5)\n",
    "      input_mask.insert(0,0)\n",
    "      segment_ids.insert(0,3)\n",
    "  while len(labels) < max_seq_length:\n",
    "      labels.insert(0,[1,0])  \n",
    "  assert len(labels) == max_seq_length    \n",
    "  assert len(input_ids) == max_seq_length\n",
    "  assert len(input_mask) == max_seq_length\n",
    "  assert len(segment_ids) == max_seq_length\n",
    "  return input_ids,input_mask,segment_ids,labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import XLNetTokenizer, TFXLNetModel,AlbertTokenizer,RobertaTokenizer\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels)=convert_examples_to_features(train_tokens, train_label, max_seq_length, tokenizer)\n",
    "(val_input_ids, val_input_masks, val_segment_ids, val_labels)=convert_examples_to_features(dev_tokens, dev_label, max_seq_length, tokenizer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import *\n",
    "import tensorflow as tf\n",
    "#tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased',force_download=True)\n",
    "model = XLNetModel.from_pretrained('xlnet-base-cased')#,force_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.xlnet=XLNetModel.from_pretrained('xlnet-base-cased')\n",
    "        #self.lstm=nn.LSTM(768, 128,\n",
    "        #                  num_layers=1, bidirectional=True,batch_first=True)\n",
    "        self.relu=nn.ReLU()\n",
    "        #self.linear1=nn.Linear(768,512)\n",
    "        self.linear2=nn.Linear(768,256)\n",
    "        self.linear3=nn.Linear(256,64)\n",
    "        self.linear=nn.Linear(64,2)\n",
    "        self.softmax=nn.LogSoftmax(2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #inputs[0]=inputs[0].to(torch.device(\"cuda:1\"))\n",
    "        #inputs[1]=inputs[1].to(torch.device(\"cuda:1\"))\n",
    "        #inputs[2]=inputs[2].to(torch.device(\"cuda:1\"))\n",
    "        x = self.xlnet(input_ids=inputs[0],attention_mask=inputs[1],token_type_ids=inputs[2])[0]\n",
    "        #x=x.to(torch.device(\"cuda:0\"))\n",
    "        #x,_ = self.lstm(x)\n",
    "        #x=self.linear1(x)\n",
    "        #x = self.relu(x)\n",
    "        x=self.linear2(x)\n",
    "        x = self.relu(x)\n",
    "        x=self.linear3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "xlnet_model=Net()\n",
    "model_clone = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlnet_model.cuda()\n",
    "criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "optimizer = optim.Adam(xlnet_model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_inputs= [torch.tensor(val_input_ids),torch.tensor(val_input_masks),torch.tensor(val_segment_ids)]\n",
    "bs=32\n",
    "top=val_input_ids.shape[0]\n",
    "max_score=0\n",
    "#xlnet_model.cuda()\n",
    "for epoch in range(30):  \n",
    "    running_loss = 0.0\n",
    "    for i in tqdm_notebook(range(math.ceil(train_input_ids.shape[0]/bs))):\n",
    "        \n",
    "        inputs= [torch.tensor(train_input_ids[i*bs:min(i*bs+bs,train_input_ids.shape[0])]).cuda(),\n",
    "                 torch.tensor(train_input_masks[i*bs:min(i*bs+bs,train_input_ids.shape[0])]).cuda(),\n",
    "                 torch.tensor(train_segment_ids[i*bs:min(i*bs+bs,train_input_ids.shape[0])]).cuda()]\n",
    "        labels = torch.tensor(train_labels[i*bs:i*bs+bs]).cuda()\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "        outputs = xlnet_model(inputs)\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        #print(\"loss \",running_loss/(i+1))\n",
    "\n",
    "        #print(f1_score(np.reshape(np.argmax(labels,axis=2),[labels.shape[0],labels.shape[1]]), \n",
    "        #                np.reshape(np.argmax(outputs.detach().numpy(),axis=2),[labels.shape[0],labels.shape[1]]),average='micro'))\n",
    "        #print('-'*10)   \n",
    "    xlnet_model.to(torch.device(\"cpu:0\"))\n",
    "    predictions=xlnet_model([torch.tensor(val_input_ids[0][np.newaxis,:]),torch.tensor(val_input_masks[0][np.newaxis,:]),torch.tensor(val_segment_ids[0][np.newaxis,:])])\n",
    "    for j in tqdm(range(math.ceil(val_input_ids.shape[0]/bs))):\n",
    "      predict=xlnet_model([torch.tensor(val_input_ids[j*bs:min(j*bs+bs,top)]),torch.tensor(val_input_masks[j*bs:min(j*bs+bs,top)]),torch.tensor(val_segment_ids[j*bs:min(j*bs+bs,top)])])\n",
    "      predictions=torch.cat((predictions,predict),0)\n",
    "    predictions=predictions[1:]    \n",
    "    score=scorer(torch.exp(predictions),dev_tokens)\n",
    "    torch.save(xlnet_model.state_dict(), \"/media/data_dump/Pradyumna/empha/pyt-D2xlnet-{}\".format(epoch))\n",
    "    if score>max_score:\n",
    "        model_clone.load_state_dict(xlnet_model.state_dict())\n",
    "        max_score=score\n",
    "    xlnet_model.cuda()\n",
    "    #print(f1_score(np.reshape(np.argmax(labels,axis=2),[labels.shape[0],labels.shape[1]]), \n",
    "    #                    np.reshape(np.argmax(predictions.detach().numpy(),axis=2),[labels.shape[0],labels.shape[1]]),average='micro'))\n",
    "    #torch.save(xlnet_model.state_dict(), \"/media/data_dump/Pradyumna/empha/pyt-temp-{}-{}.pt\".format(epoch,2e-5))\n",
    "\n",
    "    print('-'*10)\n",
    "\n",
    "\n",
    "        #if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "        #    print('[%d, %5d] loss: %.3f' %\n",
    "        #          (epoch + 1, i + 1, running_loss / 2000))\n",
    "        #    running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
