{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import pprint\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import backend as K\n",
    "from bert.tokenization import FullTokenizer\n",
    "import os \n",
    "import re\n",
    "from tqdm import tqdm_notebook,tqdm\n",
    "import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from six.moves import cPickle as pickle\n",
    "sess=tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ds_loader_label(filename):\n",
    "  with open(filename, 'r') as fp:\n",
    "    lines = [line.strip() for line in fp]   \n",
    "  posts, post = [], []\n",
    "  for line in tqdm_notebook(lines):\n",
    "    probs=[]\n",
    "    if line :\n",
    "        annotations = line.split(\"\\t\")[2]\n",
    "        # reading probabilities from the last column and also normalaize it by div on 9\n",
    "        annotations = np.array(annotations.split('|'))\n",
    "        probs.append(sum(annotations=='O'))\n",
    "        probs.append(sum(annotations=='B'))\n",
    "        probs.append(sum(annotations=='I'))\n",
    "        probs = [probs[0],probs[2]+probs[1]]\n",
    "        probs = [i/9 for i in probs]\n",
    "        post.append(probs)\n",
    "    elif post:\n",
    "        posts.append(post)\n",
    "        post = []\n",
    "  # a list of lists of words/ labels\n",
    "  return posts\n",
    "\n",
    "\n",
    "def ds_loader_token(filename):\n",
    "      with open(filename, 'r') as fp:\n",
    "        lines = [line.strip() for line in fp]   \n",
    "      posts, post = [], []\n",
    "      ids,id =[],[]  \n",
    "      for line in tqdm_notebook(lines):\n",
    "          if line:\n",
    "              words = line.split(\"\\t\")[1]\n",
    "              word_id=line.split(\"\\t\")[0]  \n",
    "              # print(\"words: \", words)\n",
    "              post.append(words)\n",
    "              id.append(word_id)  \n",
    "          elif post:\n",
    "              posts.append(post)\n",
    "              ids.append(id)  \n",
    "              id=[]  \n",
    "              post = []\n",
    "      # a list of lists of words/ labels\n",
    "      if len(post):\n",
    "            posts.append(post)\n",
    "            ids.append(id)  \n",
    "      return posts,ids\n",
    "\n",
    "train_tokens,train_ids=ds_loader_token('/home/pradyumna/emph_data/train.txt')\n",
    "train_label=ds_loader_label('/home/pradyumna/emph_data/train.txt')\n",
    "\n",
    "dev_tokens,dev_ids=ds_loader_token('/home/pradyumna/emph_data/dev.txt')\n",
    "#dev_tokens,dev_ids=ds_loader_token('/media/nas_mount/Sarthak/sarthak/semeval/shuffle_dev/shuffle_dev_5.txt')\n",
    "\n",
    "dev_label=ds_loader_label('/home/pradyumna/emph_data/dev.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_path = \"/tmp/moduleA\"  # Path to saved tensorflow hub module \n",
    "max_seq_length=max([len(token) for token in train_tokens])+2\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import backend as K\n",
    "from bert.tokenization import FullTokenizer\n",
    "import tensorflow as tf\n",
    "sess=tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id, valid_ids=None, label_mask=None):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.valid_ids = valid_ids\n",
    "        self.label_mask = label_mask\t\t\n",
    "        \n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    bert_module =  hub.Module(bert_path)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    vocab_file, do_lower_case = sess.run(\n",
    "        [\n",
    "            tokenization_info[\"vocab_file\"],\n",
    "            tokenization_info[\"do_lower_case\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "def convert_examples_to_features(tokens_set, labels_set, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    #label_map = {label: i for i, label in enumerate(label_list, 1)}\n",
    "\n",
    "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
    "    for index in tqdm_notebook(range(len(tokens_set)),desc=\"Converting examples to features\"):\n",
    "        textlist = tokens_set[index] #example.text_a.split(' ')\n",
    "        labellist = labels_set[index]\n",
    "        input_id, input_mask, segment_id,label = convert_single_example(\n",
    "            textlist, labellist,max_seq_length,tokenizer\n",
    "        )\n",
    "        input_ids.append(input_id)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "        labels.append(label)\n",
    "    return (\n",
    "        np.array(input_ids),\n",
    "        np.array(input_masks),\n",
    "        np.array(segment_ids),\n",
    "        np.array(labels)\n",
    "    )\n",
    "\n",
    "def convert_single_example(textlist, labellist, max_seq_length,tokenizer):\n",
    "  tokens = []\n",
    "  labels = []\n",
    "  for i, word in enumerate(textlist):\n",
    "      token = tokenizer.tokenize(word)\n",
    "      tokens.append(token[0])\n",
    "      labels.append(labellist[i])\n",
    "  if len(tokens) >= max_seq_length - 1:\n",
    "      tokens = tokens[0:(max_seq_length - 2)]\n",
    "      labels = labels[0:(max_seq_length - 2)]\n",
    "  ntokens = []\n",
    "  segment_ids = []\n",
    "  ntokens.append(\"[CLS]\")\n",
    "  segment_ids.append(0)\n",
    "  labels.insert(0,[1,0])\n",
    "  for i, token in enumerate(tokens):\n",
    "      ntokens.append(token)\n",
    "      segment_ids.append(0)\n",
    "  ntokens.append(\"[SEP]\")\n",
    "  segment_ids.append(0)\n",
    "  labels.append([1,0])\n",
    "  input_ids = tokenizer.convert_tokens_to_ids(ntokens)\n",
    "  input_mask = [1] * len(input_ids)\n",
    "  while len(input_ids) < max_seq_length:\n",
    "      input_ids.append(0)\n",
    "      input_mask.append(0)\n",
    "      segment_ids.append(0)\n",
    "  while len(labels) < max_seq_length:\n",
    "      labels.append([1,0])  \n",
    "  assert len(labels) == max_seq_length    \n",
    "  assert len(input_ids) == max_seq_length\n",
    "  assert len(input_mask) == max_seq_length\n",
    "  assert len(segment_ids) == max_seq_length\n",
    "  return input_ids,input_mask,segment_ids,labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer_from_hub_module()\n",
    "\n",
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels)=convert_examples_to_features(train_tokens, train_label, max_seq_length, tokenizer)\n",
    "(val_input_ids, val_input_masks, val_segment_ids, val_labels)=convert_examples_to_features(dev_tokens, dev_label, max_seq_length, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    true=K.cast(K.flatten(K.argmax(y_true,axis=2)),dtype='float32')\n",
    "    pred=K.cast(K.flatten(K.argmax(y_pred,axis=2)),dtype='float32')\n",
    "    precision = precision(true, pred)\n",
    "    recall = recall(true, pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "\n",
    "def average(lst):\n",
    "    return sum(lst) / float(len(lst))\n",
    "\n",
    "\n",
    "def intersection(lst1, lst2):\n",
    "    return list(set(lst1) & set(lst2))\n",
    "\n",
    "\n",
    "def match_m(all_scores, all_labels):\n",
    "    \"\"\"\n",
    "    This function computes match_m.\n",
    "    :param all_scores: submission scores\n",
    "    :param all_labels: ground_truth labels\n",
    "    :return: match_m dict\n",
    "    \"\"\"\n",
    "    print(\"[LOG] computing Match_m . . .\")\n",
    "    top_m = [1, 2, 3, 4]\n",
    "    match_ms = {}\n",
    "    for m in top_m:\n",
    "        print(\"[LOG] computing m={} in match_m\".format(m))\n",
    "        intersects_lst = []\n",
    "        # ****************** computing scores:\n",
    "        score_lst = []\n",
    "        for s in all_scores:\n",
    "            # the length of sentence needs to be more than m:\n",
    "            if len(s) <= m:\n",
    "                continue\n",
    "            s = np.array(s)\n",
    "            ind_score = np.argsort(s)[-m:]\n",
    "            score_lst.append(ind_score.tolist())\n",
    "        # ****************** computing labels:\n",
    "        label_lst = []\n",
    "        for l in all_labels:\n",
    "            # the length of sentence needs to be more than m:\n",
    "            if len(l) <= m:\n",
    "                continue\n",
    "            # if label list contains several top values with the same amount we consider them all\n",
    "            h = m\n",
    "            if len(l) > h:\n",
    "                while (l[np.argsort(l)[-h]] == l[np.argsort(l)[-(h + 1)]] and h < (len(l) - 1)):\n",
    "                    h += 1\n",
    "            l = np.array(l)\n",
    "            ind_label = np.argsort(l)[-h:]\n",
    "            label_lst.append(ind_label.tolist())\n",
    "\n",
    "        for i in range(len(score_lst)):\n",
    "            # computing the intersection between scores and ground_truth labels:\n",
    "            intersect = intersection(score_lst[i], label_lst[i])\n",
    "            intersects_lst.append((len(intersect))/float((min(m, len(score_lst[i])))))\n",
    "        # taking average of intersects for the current m:\n",
    "        match_ms[m] = average(intersects_lst)\n",
    "\n",
    "    return match_ms\n",
    "\n",
    "\n",
    "def read_results(filename):\n",
    "    lines = read_lines(filename) + ['']\n",
    "    e_freq_lst, e_freq_lsts = [], []\n",
    "\n",
    "    for line in lines:\n",
    "        if line:\n",
    "            splitted = line.split(\"\\t\")\n",
    "            e_freq = splitted[2]\n",
    "            e_freq_lst.append(e_freq)\n",
    "\n",
    "        elif e_freq_lst:\n",
    "            e_freq_lsts.append(e_freq_lst)\n",
    "            e_freq_lst = []\n",
    "    return e_freq_lsts\n",
    "\n",
    "\n",
    "def read_labels(filename):\n",
    "    lines = read_lines(filename) + ['']\n",
    "    e_freq_lst, e_freq_lsts = [], []\n",
    "\n",
    "    for line in lines:\n",
    "        if line:\n",
    "            splitted = line.split(\"\\t\")\n",
    "            e_freq = splitted[4]\n",
    "            e_freq_lst.append(e_freq)\n",
    "\n",
    "        elif e_freq_lst:\n",
    "            e_freq_lsts.append(e_freq_lst)\n",
    "            e_freq_lst = []\n",
    "    return e_freq_lsts\n",
    "\n",
    "\n",
    "def read_lines(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        lines = [line.strip() for line in fp]\n",
    "    return lines\n",
    "\n",
    "\n",
    "def scorer(predictions,dev_tokens,dev_ids):\n",
    "    with open(\"/home/pradyumna/emph_data/prediction.txt\",'w') as f:\n",
    "        for i,example in tqdm_notebook(enumerate(dev_tokens)):\n",
    "            for j,token in enumerate(example):\n",
    "                f.write(dev_ids[i][j]+'\\t'+token+'\\t'+str(predictions[i][j+1][1])+'\\t'+'\\n')\n",
    "            f.write('\\n')    \n",
    "        \n",
    "    all_score = read_results(\"/home/pradyumna/emph_data/prediction.txt\")# Path to text file where predictions will be saved \n",
    "    all_label = read_labels(\"/home/pradyumna/emph_data/dev.txt\") # Path of Dev File\n",
    "\n",
    "    assert len(all_score) == len(all_label)\n",
    "    for i in range(len(all_label)):\n",
    "        assert len(all_label[i]) == len(all_score[i])\n",
    "\n",
    "    matchm = match_m(all_score, all_label)\n",
    "    print(\"[LOG] Match_m: \", matchm)\n",
    "    print(\"[LOG] computing RANKING score\")\n",
    "\n",
    "    sum_of_all_scores = 0\n",
    "    for key,value in matchm.items():\n",
    "        #output_file.write(\"score\"+str(key)+\":\"+str(value))\n",
    "        #output_file.write(\"\\n\")\n",
    "        sum_of_all_scores+=value\n",
    "    print(\"score:\"+str(sum_of_all_scores/float(4))+\"\\n\") #score for final \"computed score\"\n",
    "    \n",
    "    return sum_of_all_scores/float(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "  def __init__(\n",
    "      self,\n",
    "      n_fine_tune_layers=10,\n",
    "      pooling=\"first\",\n",
    "      bert_path=bert_path,\n",
    "      **kwargs,\n",
    "  ):\n",
    "      self.n_fine_tune_layers = n_fine_tune_layers\n",
    "      self.trainable = True\n",
    "      self.output_size = 768\n",
    "      self.pooling = pooling\n",
    "      self.bert_path = bert_path\n",
    "      if self.pooling not in [\"first\", \"mean\"]:\n",
    "          raise NameError(\n",
    "              f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "          )\n",
    "\n",
    "      super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "  def build(self, input_shape):\n",
    "      self.bert = hub.Module(\n",
    "          self.bert_path, trainable=self.trainable, name=f\"{self.name}_module\"\n",
    "      )\n",
    "\n",
    "      # Remove unused layers\n",
    "      trainable_vars = self.bert.variables\n",
    "      if self.pooling == \"first\":\n",
    "          trainable_vars = [\n",
    "              var\n",
    "              for var in trainable_vars\n",
    "              if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "          ]\n",
    "          trainable_layers = []\n",
    "\n",
    "      elif self.pooling == \"mean\":\n",
    "          trainable_vars = [\n",
    "              var\n",
    "              for var in trainable_vars\n",
    "              if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "          ]\n",
    "          trainable_layers = []\n",
    "      else:\n",
    "          raise NameError(\n",
    "              f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "          )\n",
    "\n",
    "      # Select how many layers to fine tune\n",
    "      for i in range(self.n_fine_tune_layers):\n",
    "          trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "\n",
    "      # Update trainable vars to contain only the specified layers\n",
    "      trainable_vars = [\n",
    "          var\n",
    "          for var in trainable_vars\n",
    "          if any([l in var.name for l in trainable_layers])\n",
    "      ]\n",
    "\n",
    "      # Add to trainable weights\n",
    "      for var in trainable_vars:\n",
    "          self._trainable_weights.append(var)\n",
    "\n",
    "      for var in self.bert.variables:\n",
    "          if var not in self._trainable_weights:\n",
    "              self._non_trainable_weights.append(var)\n",
    "\n",
    "      super(BertLayer, self).build(input_shape)\n",
    "\n",
    "  def call(self, inputs):\n",
    "      inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "      input_ids, input_mask, segment_ids = inputs\n",
    "      bert_inputs = dict(\n",
    "          input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "      )\n",
    "      if self.pooling == \"first\":\n",
    "          pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "              \"sequence_output\"\n",
    "          ]\n",
    "          #print(pooled.s)\n",
    "      elif self.pooling == \"mean\":\n",
    "          result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "              \"sequence_output\"\n",
    "          ]\n",
    "\n",
    "          mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "          masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
    "                  tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "          input_mask = tf.cast(input_mask, tf.float32)\n",
    "          pooled = masked_reduce_mean(result, input_mask)\n",
    "      else:\n",
    "          raise NameError(f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\")\n",
    "      #pooled=K.reshape(pooled,(None,72,768))\n",
    "      return pooled\n",
    "      #return valid_output\n",
    "\n",
    "  def compute_output_shape(self, input_shape):\n",
    "      return (input_shape[0], self.output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input,Dense,Bidirectional,CuDNNLSTM,LSTM\n",
    "\n",
    "\n",
    "for lr in [3e-4]:#,2e-5,3e-3]:\n",
    "    for epochs in [3,5,10,13]:\n",
    "        for dropout in [0]:#,0.1,0.3]:#,0.3,0.1]:\n",
    "            for layers in [1,2,3]:#,2,3]:\n",
    "              \n",
    "\n",
    "              in_id = tf.keras.layers.Input(shape=(max_seq_length,))\n",
    "              in_mask = tf.keras.layers.Input(shape=(max_seq_length,))\n",
    "              in_segment = tf.keras.layers.Input(shape=(max_seq_length,))\n",
    "\n",
    "              bert_inputs=[in_id,in_mask,in_segment]\n",
    "\n",
    "              bert_outputs=BertLayer(n_fine_tune_layers=1,pooling='first')(bert_inputs)\n",
    "              step=bert_outputs\n",
    "\n",
    "              if layers>=3:\n",
    "                  step=tf.keras.layers.Dense(512,activation='relu')(step)\n",
    "                  if dropout!=0:\n",
    "                      step=tf.keras.layers.Dropout(rate=dropout)(step)\n",
    "              if layers>=2:\n",
    "                  step=tf.keras.layers.Dense(256,activation='relu')(step)\n",
    "                  if dropout!=0:\n",
    "                      step=tf.keras.layers.Dropout(rate=dropout)(step)\n",
    "              if layers>=1:    \n",
    "                  step=tf.keras.layers.Dense(64,activation='relu')(step)\n",
    "                  if dropout!=0:\n",
    "                      step=tf.keras.layers.Dropout(rate=dropout)(step)    \n",
    "              \n",
    "              #step=Bidirectional(LSTM(128,return_sequences=True,activation='relu'))(step)\n",
    "                \n",
    "              step=tf.keras.layers.Dense(2,activation='softmax')(step)\n",
    "\n",
    "              #crf = CRF(2,learn_mode='marginal')              \n",
    "              #step = crf(step)  \n",
    "                \n",
    "\n",
    "              model=tf.keras.Model(inputs=bert_inputs,outputs=step)\n",
    "              \n",
    "              model.compile(loss='kullback_leibler_divergence',\n",
    "              optimizer=tf.keras.optimizers.Adam(lr=lr),\n",
    "              metrics=[f1,'accuracy'])\n",
    "              \n",
    "              model.summary();\n",
    "\n",
    "              sess.run(tf.local_variables_initializer())\n",
    "              sess.run(tf.global_variables_initializer())\n",
    "              sess.run(tf.tables_initializer())\n",
    "              K.set_session(sess)        \n",
    "\n",
    "              model.fit([train_input_ids, train_input_masks, train_segment_ids],\n",
    "                                      train_labels,\n",
    "                                      epochs=epochs,\n",
    "                                      batch_size=32,\n",
    "                                      validation_data=([val_input_ids, val_input_masks, val_segment_ids],val_labels)\n",
    "                                      #class_weight=dict(enumerate(class_weights))  \n",
    "                                      #callbacks=[\n",
    "                                          #tf.keras.caltlbacks.LearningRateScheduler(scheduler,verbose=1)\n",
    "                                          #TensorBoardColabCallback(tbc)\n",
    "                                      #    Saver()\n",
    "                                      #  ] \n",
    "                                      )\n",
    "              \n",
    "              predictions=model.predict([val_input_ids, val_input_masks, val_segment_ids])\n",
    "              scorer(predictions,dev_tokens,dev_ids)  \n",
    "              print(f1_score(np.reshape(np.argmax(val_labels,axis=2),[val_labels.shape[0],val_labels.shape[1]]), \n",
    "                            np.reshape(np.argmax(predictions,axis=2),[val_labels.shape[0],val_labels.shape[1]]),average='micro'))\n",
    "              print(f1_score(np.reshape(np.argmax(val_labels,axis=2),[val_labels.shape[0],val_labels.shape[1]]), \n",
    "                            np.reshape(np.argmax(predictions,axis=2),[val_labels.shape[0],val_labels.shape[1]]),average='macro'))          \n",
    "              #model.save('/media/data_dump/Pradyumna/empha/model-{}-{}-{}-{}-12.h5'.format(lr,epochs,dropout,layers))\n",
    "              model_json = model.to_json()\n",
    "              with open('/media/data_dump/Pradyumna/empha/Dmodel-{}-{}-{}-{}-L.json'.format(lr,epochs,dropout,layers), \"w\") as json_file:\n",
    "                json_file.write(model_json)\n",
    "                # serialize weights to HDF5\n",
    "              model.save_weights('/media/data_dump/Pradyumna/empha/Dmodel-{}-{}-{}-{}-L.h5'.format(lr,epochs,dropout,layers))  \n",
    "              print(\"Done!!\")  \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Saver(tf.keras.callbacks.Callback):\n",
    "  def on_train_begin(self,logs={}):\n",
    "    self.score=0\n",
    "  def on_epoch_end(self,logs={},*args):\n",
    "    #self.model.save_weights('/media/data_dump/Pradyumna/empha/model-{}-{}-{}-{}-L.h5'.format(lr,epochs,dropout,layers))  \n",
    "    predictions=self.model.predict([val_input_ids, val_input_masks, val_segment_ids])\n",
    "    res=scorer(predictions,dev_tokens)\n",
    "    if res>self.score:\n",
    "        self.model.save_weights('/media/data_dump/Pradyumna/empha/model-{}-{}-{}-{}-L.h5'.format(lr,epochs,dropout,layers))  \n",
    "    #bert_vars={}\n",
    "    #for i in tqdm_notebook(range(len(self.model.layers[3].weights))):\n",
    "    #    name=self.model.layers[3].weights[i].name\n",
    "    #    array=self.model.layers[3].get_weights()[i]\n",
    "    #    bert_vars[name]=array\n",
    "    #with open('/media/nas_mount/Sarthak/emphasis_weights/bert-L-2e-05-{}.pkl'.format(self.counter), 'wb') as f:\n",
    "    #    pickle.dump(bert_vars, f)     \n",
    "    #self.counter=self.counter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"/media/data_dump/Pradyumna/empha/Dmodel-0.0003-3-0-2.h5\")\n",
    "predictions=model.predict([val_input_ids, val_input_masks, val_segment_ids])\n",
    "scorer(predictions,dev_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
