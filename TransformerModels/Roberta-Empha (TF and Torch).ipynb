{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import pprint\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import os \n",
    "import re\n",
    "from tqdm import tqdm_notebook,tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ds_loader_label(filename):\n",
    "  with open(filename, 'r') as fp:\n",
    "    lines = [line.strip() for line in fp]   \n",
    "  posts, post = [], []\n",
    "  for line in tqdm_notebook(lines):\n",
    "    probs=[]\n",
    "    if line :\n",
    "        annotations = line.split(\"\\t\")[2]\n",
    "        # reading probabilities from the last column and also normalaize it by div on 9\n",
    "        annotations = np.array(annotations.split('|'))\n",
    "        probs.append(sum(annotations=='O'))\n",
    "        probs.append(sum(annotations=='B'))\n",
    "        probs.append(sum(annotations=='I'))\n",
    "        probs = [probs[0],probs[2]+probs[1]]\n",
    "        probs = [i/9 for i in probs]\n",
    "        post.append(probs)\n",
    "    elif post:\n",
    "        posts.append(post)\n",
    "        post = []\n",
    "  # a list of lists of words/ labels\n",
    "  return posts\n",
    "\n",
    "\n",
    "def ds_loader_token(filename):\n",
    "      with open(filename, 'r') as fp:\n",
    "        lines = [line.strip() for line in fp]   \n",
    "      posts, post = [], []\n",
    "      ids,id =[],[]  \n",
    "      for line in tqdm_notebook(lines):\n",
    "          if line:\n",
    "              words = line.split(\"\\t\")[1]\n",
    "              word_id=line.split(\"\\t\")[0]  \n",
    "              # print(\"words: \", words)\n",
    "              post.append(words)\n",
    "              id.append(word_id)  \n",
    "          elif post:\n",
    "              posts.append(post)\n",
    "              ids.append(id)  \n",
    "              id=[]  \n",
    "              post = []\n",
    "      # a list of lists of words/ labels\n",
    "      if len(post):\n",
    "        posts.append(post)\n",
    "        ids.append(id)  \n",
    "      return posts,ids\n",
    "\n",
    "train_tokens,train_ids=ds_loader_token('/home/pradyumna/emph_data/train.txt')\n",
    "train_label=ds_loader_label('/home/pradyumna/emph_data/train.txt')\n",
    "\n",
    "dev_tokens,dev_ids=ds_loader_token('/home/pradyumna/emph_data/dev.txt')\n",
    "#dev_tokens,dev_ids=ds_loader_token('/media/nas_mount/Sarthak/sarthak/semeval/shuffle_dev/shuffle_dev_5.txt')\n",
    "\n",
    "dev_label=ds_loader_label('/home/pradyumna/emph_data/dev.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length=max([len(token) for token in train_tokens])+2\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id, valid_ids=None, label_mask=None):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.valid_ids = valid_ids\n",
    "        self.label_mask = label_mask\t\t\n",
    "        \n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    bert_module =  hub.Module(bert_path)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    vocab_file, do_lower_case = sess.run(\n",
    "        [\n",
    "            tokenization_info[\"vocab_file\"],\n",
    "            tokenization_info[\"do_lower_case\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "def convert_examples_to_features(tokens_set, labels_set, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    #label_map = {label: i for i, label in enumerate(label_list, 1)}\n",
    "\n",
    "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
    "    for index in tqdm_notebook(range(len(tokens_set)),desc=\"Converting examples to features\"):\n",
    "        textlist = tokens_set[index] #example.text_a.split(' ')\n",
    "        labellist = labels_set[index]\n",
    "        input_id, input_mask, segment_id,label = convert_single_example(\n",
    "            textlist, labellist,max_seq_length,tokenizer\n",
    "        )\n",
    "        input_ids.append(input_id)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "        labels.append(label)\n",
    "    return (\n",
    "        np.array(input_ids),\n",
    "        np.array(input_masks),\n",
    "        np.array(segment_ids),\n",
    "        np.array(labels)\n",
    "    )\n",
    "\n",
    "def convert_single_example(textlist, labellist, max_seq_length,tokenizer):\n",
    "  tokens = []\n",
    "  labels = []\n",
    "  for i, word in enumerate(textlist):\n",
    "      #token = tokenizer.tokenize(word)\n",
    "      if i>0 :\n",
    "        token = tokenizer.tokenize(\"i \"+word)  \n",
    "        tokens.append(token[1])\n",
    "      else :\n",
    "        token = tokenizer.tokenize(word)\n",
    "        tokens.append(token[0])\n",
    "      labels.append(labellist[i])\n",
    "  if len(tokens) >= max_seq_length - 1:\n",
    "      tokens = tokens[0:(max_seq_length - 2)]\n",
    "      labels = labels[0:(max_seq_length - 2)]\n",
    "  ntokens = []\n",
    "  segment_ids = []\n",
    "  #ntokens.append(\"[CLS]\")\n",
    "  segment_ids.append(0)\n",
    "  labels.insert(0,[1,0])\n",
    "  for i, token in enumerate(tokens):\n",
    "      ntokens.append(token)\n",
    "      segment_ids.append(0)\n",
    "  #ntokens.append(\"[SEP]\")\n",
    "  segment_ids.append(0)\n",
    "  labels.append([1,0])\n",
    "  input_ids = tokenizer.convert_tokens_to_ids(ntokens)\n",
    "  input_ids.insert(0,0)\n",
    "  input_ids.append(2)  \n",
    "  input_mask = [1] * len(input_ids)\n",
    "  while len(input_ids) < max_seq_length:\n",
    "      input_ids.append(1)\n",
    "      input_mask.append(0)\n",
    "      segment_ids.append(0)\n",
    "  while len(labels) < max_seq_length:\n",
    "      labels.append([1,0])  \n",
    "  assert len(labels) == max_seq_length    \n",
    "  assert len(input_ids) == max_seq_length\n",
    "  assert len(input_mask) == max_seq_length\n",
    "  assert len(segment_ids) == max_seq_length\n",
    "  return input_ids,input_mask,segment_ids,labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import XLNetTokenizer, TFXLNetModel,AlbertTokenizer,RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels)=convert_examples_to_features(train_tokens, train_label, max_seq_length, tokenizer)\n",
    "(val_input_ids, val_input_masks, val_segment_ids, val_labels)=convert_examples_to_features(dev_tokens, dev_label, max_seq_length, tokenizer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    true=K.cast(K.flatten(K.argmax(y_true,axis=2)),dtype='float32')\n",
    "    pred=K.cast(K.flatten(K.argmax(y_pred,axis=2)),dtype='float32')\n",
    "    precision = precision(true, pred)\n",
    "    recall = recall(true, pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "\n",
    "def average(lst):\n",
    "    return sum(lst) / float(len(lst))\n",
    "\n",
    "\n",
    "def intersection(lst1, lst2):\n",
    "    return list(set(lst1) & set(lst2))\n",
    "\n",
    "\n",
    "def match_m(all_scores, all_labels):\n",
    "    \"\"\"\n",
    "    This function computes match_m.\n",
    "    :param all_scores: submission scores\n",
    "    :param all_labels: ground_truth labels\n",
    "    :return: match_m dict\n",
    "    \"\"\"\n",
    "    print(\"[LOG] computing Match_m . . .\")\n",
    "    top_m = [1, 2, 3, 4]\n",
    "    match_ms = {}\n",
    "    for m in top_m:\n",
    "        print(\"[LOG] computing m={} in match_m\".format(m))\n",
    "        intersects_lst = []\n",
    "        # ****************** computing scores:\n",
    "        score_lst = []\n",
    "        for s in all_scores:\n",
    "            # the length of sentence needs to be more than m:\n",
    "            if len(s) <= m:\n",
    "                continue\n",
    "            s = np.array(s)\n",
    "            ind_score = np.argsort(s)[-m:]\n",
    "            score_lst.append(ind_score.tolist())\n",
    "        # ****************** computing labels:\n",
    "        label_lst = []\n",
    "        for l in all_labels:\n",
    "            # the length of sentence needs to be more than m:\n",
    "            if len(l) <= m:\n",
    "                continue\n",
    "            # if label list contains several top values with the same amount we consider them all\n",
    "            h = m\n",
    "            if len(l) > h:\n",
    "                while (l[np.argsort(l)[-h]] == l[np.argsort(l)[-(h + 1)]] and h < (len(l) - 1)):\n",
    "                    h += 1\n",
    "            l = np.array(l)\n",
    "            ind_label = np.argsort(l)[-h:]\n",
    "            label_lst.append(ind_label.tolist())\n",
    "\n",
    "        for i in range(len(score_lst)):\n",
    "            # computing the intersection between scores and ground_truth labels:\n",
    "            intersect = intersection(score_lst[i], label_lst[i])\n",
    "            intersects_lst.append((len(intersect))/float((min(m, len(score_lst[i])))))\n",
    "        # taking average of intersects for the current m:\n",
    "        match_ms[m] = average(intersects_lst)\n",
    "\n",
    "    return match_ms\n",
    "\n",
    "\n",
    "def read_results(filename):\n",
    "    lines = read_lines(filename) + ['']\n",
    "    e_freq_lst, e_freq_lsts = [], []\n",
    "\n",
    "    for line in lines:\n",
    "        if line:\n",
    "            splitted = line.split(\"\\t\")\n",
    "            e_freq = splitted[2]\n",
    "            e_freq_lst.append(e_freq)\n",
    "\n",
    "        elif e_freq_lst:\n",
    "            e_freq_lsts.append(e_freq_lst)\n",
    "            e_freq_lst = []\n",
    "    return e_freq_lsts\n",
    "\n",
    "\n",
    "def read_labels(filename):\n",
    "    lines = read_lines(filename) + ['']\n",
    "    e_freq_lst, e_freq_lsts = [], []\n",
    "\n",
    "    for line in lines:\n",
    "        if line:\n",
    "            splitted = line.split(\"\\t\")\n",
    "            e_freq = splitted[4]\n",
    "            e_freq_lst.append(e_freq)\n",
    "\n",
    "        elif e_freq_lst:\n",
    "            e_freq_lsts.append(e_freq_lst)\n",
    "            e_freq_lst = []\n",
    "    return e_freq_lsts\n",
    "\n",
    "\n",
    "def read_lines(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        lines = [line.strip() for line in fp]\n",
    "    return lines\n",
    "\n",
    "\n",
    "def scorer(predictions,dev_tokens,dev_ids):\n",
    "    with open(\"/home/pradyumna/emph_data/prediction.txt\",'w') as f:\n",
    "        for i,example in tqdm_notebook(enumerate(dev_tokens)):\n",
    "            for j,token in enumerate(example):\n",
    "                f.write(dev_ids[i][j]+'\\t'+token+'\\t'+str(predictions[i][j+1][1])+'\\t'+'\\n')\n",
    "            f.write('\\n')    \n",
    "        \n",
    "    all_score = read_results(\"/home/pradyumna/emph_data/prediction.txt\")\n",
    "    all_label = read_labels(\"/home/pradyumna/emph_data/dev.txt\")\n",
    "\n",
    "    assert len(all_score) == len(all_label)\n",
    "    for i in range(len(all_label)):\n",
    "        assert len(all_label[i]) == len(all_score[i])\n",
    "\n",
    "    matchm = match_m(all_score, all_label)\n",
    "    print(\"[LOG] Match_m: \", matchm)\n",
    "    print(\"[LOG] computing RANKING score\")\n",
    "\n",
    "    sum_of_all_scores = 0\n",
    "    for key,value in matchm.items():\n",
    "        #output_file.write(\"score\"+str(key)+\":\"+str(value))\n",
    "        #output_file.write(\"\\n\")\n",
    "        sum_of_all_scores+=value\n",
    "    print(\"score:\"+str(sum_of_all_scores/float(4))+\"\\n\") #score for final \"computed score\"\n",
    "    \n",
    "    return sum_of_all_scores/float(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input,Dense,Bidirectional,LSTM\n",
    "\n",
    "for lr in [2e-5]:\n",
    "    for epochs in [3,5,7,10]:\n",
    "        for layer in [3]:#[1,2,3]:\n",
    "            for dropout in [0]:#,0.1,0.3]:\n",
    "                tf.keras.backend.clear_session()\n",
    "                \n",
    "                token_inputs = tf.keras.layers.Input(shape=(None,), name='word_inputs', dtype=tf.int32)\n",
    "                mask_inputs = tf.keras.layers.Input(shape=(None,), name='mask_inputs', dtype=tf.int32)\n",
    "                seg_inputs = tf.keras.layers.Input(shape=(None,), name='seg_inputs', dtype=tf.int32)\n",
    "\n",
    "                inputs=[token_inputs,mask_inputs,seg_inputs]\n",
    "\n",
    "                transformer_outputs= TFRobertaModel.from_pretrained('roberta-base')(inputs)[0]\n",
    "\n",
    "                #albert_outputs=K.squeeze(albert_last_outputs[:, 0:1, :], axis=1)\n",
    "\n",
    "                step=transformer_outputs\n",
    "                \n",
    "                #step=tf.keras.layers.BatchNormalization()(step)\n",
    "                if layer>=3:\n",
    "                  step=tf.keras.layers.Dense(512,activation='relu')(step)\n",
    "                  if dropout!=0:\n",
    "                      step=tf.keras.layers.Dropout(rate=dropout)(step)\n",
    "                if layer>=2:\n",
    "                  step=tf.keras.layers.Dense(256,activation='relu')(step)\n",
    "                  if dropout!=0:\n",
    "                      step=tf.keras.layers.Dropout(rate=dropout)(step)\n",
    "                if layer>=1:    \n",
    "                  step=tf.keras.layers.Dense(64,activation='relu')(step)\n",
    "                  if dropout!=0:\n",
    "                      step=tf.keras.layers.Dropout(rate=dropout)(step)    \n",
    "                #step=tf.keras.layers.Dropout(rate=dropout)(step)            \n",
    "                \n",
    "                #step=Bidirectional(LSTM(128,return_sequences=True,activation='relu'))(step)\n",
    "\n",
    "\n",
    "                pred=tf.keras.layers.Dense(2,activation='softmax')(step)\n",
    "\n",
    "                model=tf.keras.Model(inputs=inputs,outputs=pred)\n",
    "\n",
    "                #model.layers[3].trainable=False\n",
    "\n",
    "\n",
    "                model.compile(loss='kullback_leibler_divergence',\n",
    "                      optimizer=tf.keras.optimizers.Adam(lr=lr),\n",
    "                      metrics=[f1,'accuracy'])\n",
    "                model.summary()\n",
    "\n",
    "                #from sklearn.utils.class_weight import compute_class_weight\n",
    "                #class_weights = compute_class_weight('balanced', np.unique(data_train.iloc[:,1]), data_train.iloc[:,1])\n",
    "\n",
    "                model.fit([train_input_ids,train_input_masks,train_segment_ids],\n",
    "                      train_labels,\n",
    "                      epochs=epochs,\n",
    "                      batch_size=32,\n",
    "                      validation_data=([val_input_ids,val_input_masks,val_segment_ids],val_labels),\n",
    "                      #class_weight=dict(enumerate(class_weights))  \n",
    "                      #callbacks=[\n",
    "                          #tf.keras.caltlbacks.LearningRateScheduler(scheduler,verbose=1)\n",
    "                          #TensorBoardColabCallback(tbc)\n",
    "                      #  ] \n",
    "                        )\n",
    "                predictions=model.predict([val_input_ids, val_input_masks, val_segment_ids])\n",
    "                scorer(predictions,dev_tokens,dev_ids)  \n",
    "                print(f1_score(np.reshape(np.argmax(val_labels,axis=2),[val_labels.shape[0],val_labels.shape[1]]), \n",
    "                            np.reshape(np.argmax(predictions,axis=2),[val_labels.shape[0],val_labels.shape[1]]),average='micro'))\n",
    "                print(f1_score(np.reshape(np.argmax(val_labels,axis=2),[val_labels.shape[0],val_labels.shape[1]]), \n",
    "                            np.reshape(np.argmax(predictions,axis=2),[val_labels.shape[0],val_labels.shape[1]]),average='macro'))          \n",
    "                #model.save('/media/data_dump/Pradyumna/empha/model-{}-{}-{}-{}-12.h5'.format(lr,epochs,dropout,layers))\n",
    "                #model_json = model.to_json()\n",
    "                #with open('/media/data_dump/Pradyumna/empha/roberta-{}-{}-{}-{}.json'.format(lr,epochs), \"w\") as json_file:\n",
    "                #    json_file.write(model_json)\n",
    "                # serialize weights to HDF5\n",
    "                model.save_weights('/media/data_dump/Pradyumna/empha/Droberta-{}-{}-{}-{}.h5'.format(lr,epochs,layer,dropout))  \n",
    "                print(\"Done!!\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import *\n",
    "import tensorflow as tf\n",
    "#tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased',force_download=True)\n",
    "model = RobertaModel.from_pretrained('roberta-base')#,force_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.roberta=model\n",
    "        self.lstm=nn.LSTM(768, 128,\n",
    "                          num_layers=1, bidirectional=True,batch_first=True)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.linear=nn.Linear(256,2)\n",
    "        self.softmax=nn.LogSoftmax(2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #inputs[0]=inputs[0].to(torch.device(\"cuda:1\"))\n",
    "        #inputs[1]=inputs[1].to(torch.device(\"cuda:1\"))\n",
    "        #inputs[2]=inputs[2].to(torch.device(\"cuda:1\"))\n",
    "        x = self.roberta(input_ids=inputs[0],attention_mask=inputs[1],token_type_ids=inputs[2])[0]\n",
    "        #x=x.to(torch.device(\"cuda:0\"))\n",
    "        x,_ = self.lstm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "roberta_model=Net()\n",
    "model_clone = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Block To Freeze Some Layers of the Roberta Model\n",
    "\n",
    "\n",
    "\n",
    "#for name, param in XLNetModel.from_pretrained('xlnet-base-cased').named_parameters():                \n",
    "#    if param.requires_grad:\n",
    "#        print(name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for name, param in roberta_model.named_parameters():                \n",
    "    if \"roberta\" in name and \"11\" not in name:\n",
    "        param.requires_grad=False\n",
    "    if \"roberta\" in name and \"11\" in name:\n",
    "        param.requires_grad=True\n",
    "\n",
    "for name, param in roberta_model.named_parameters():                \n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_model.cuda()\n",
    "criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "optimizer = optim.Adam(roberta_model.parameters(), lr=9e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_inputs= [torch.tensor(val_input_ids),torch.tensor(val_input_masks),torch.tensor(val_segment_ids)]\n",
    "bs=32\n",
    "top=val_input_ids.shape[0]\n",
    "max_score=0\n",
    "for epoch in range(20):  \n",
    "    running_loss = 0.0\n",
    "    for i in tqdm_notebook(range(math.ceil(train_input_ids.shape[0]/bs))):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs= [torch.tensor(train_input_ids[i*bs:min(i*bs+bs,train_input_ids.shape[0])]).cuda(),\n",
    "                 torch.tensor(train_input_masks[i*bs:min(i*bs+bs,train_input_ids.shape[0])]).cuda(),\n",
    "                 torch.tensor(train_segment_ids[i*bs:min(i*bs+bs,train_input_ids.shape[0])]).cuda()]\n",
    "        labels = torch.tensor(train_labels[i*bs:i*bs+bs]).cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = roberta_model(inputs)\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        #print(\"loss \",running_loss/(i+1))\n",
    "\n",
    "        #print(f1_score(np.reshape(np.argmax(labels,axis=2),[labels.shape[0],labels.shape[1]]), \n",
    "        #                np.reshape(np.argmax(outputs.detach().numpy(),axis=2),[labels.shape[0],labels.shape[1]]),average='micro'))\n",
    "        #print('-'*10)   \n",
    "    roberta_model.to(torch.device(\"cpu:0\"))\n",
    "    torch.save(roberta_model.state_dict(), \"/media/data_dump/Pradyumna/empha/L_R-{}\".format(epoch))\n",
    "    predictions=roberta_model([torch.tensor(val_input_ids[0][np.newaxis,:]),torch.tensor(val_input_masks[0][np.newaxis,:]),torch.tensor(val_segment_ids[0][np.newaxis,:])])\n",
    "    for j in tqdm(range(math.ceil(val_input_ids.shape[0]/bs))):\n",
    "      predict=roberta_model([torch.tensor(val_input_ids[j*bs:min(j*bs+bs,top)]),torch.tensor(val_input_masks[j*bs:min(j*bs+bs,top)]),torch.tensor(val_segment_ids[j*bs:min(j*bs+bs,top)])])\n",
    "      predictions=torch.cat((predictions,predict),0)\n",
    "    predictions=predictions[1:]    \n",
    "    score=scorer(torch.exp(predictions),dev_tokens)\n",
    "    if score>max_score:\n",
    "        model_clone.load_state_dict(roberta_model.state_dict())\n",
    "        max_score=score\n",
    "    roberta_model.cuda()\n",
    "    #print(f1_score(np.reshape(np.argmax(labels,axis=2),[labels.shape[0],labels.shape[1]]), \n",
    "    #                    np.reshape(np.argmax(predictions.detach().numpy(),axis=2),[labels.shape[0],labels.shape[1]]),average='micro'))\n",
    "    #torch.save(xlnet_model.state_dict(), \"/media/data_dump/Pradyumna/empha/pyt-temp-{}-{}.pt\".format(epoch,2e-5))\n",
    "\n",
    "    print('-'*10)\n",
    "\n",
    "\n",
    "        #if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "        #    print('[%d, %5d] loss: %.3f' %\n",
    "        #          (epoch + 1, i + 1, running_loss / 2000))\n",
    "        #    running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_model.load_state_dict(torch.load(\"/media/data_dump/Pradyumna/empha/L_R-5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
